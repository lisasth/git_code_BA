{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8cfec8",
   "metadata": {},
   "source": [
    "# Pre-processing of Data for Animacy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d5258",
   "metadata": {},
   "source": [
    "## Files for Classifier A\n",
    "\n",
    "First, 100 randomly chosen files out of the 25.000 files are stored.\n",
    "\n",
    "The original files in the WebAnno TSV format are processed and written with a new header. Also, a new column is added to the TSV files and with the help of a noun chunker and the IOB format, the nouns in the texts are labeled (B = begin of a noun chunk, I = not the begin of a noun chunk). Both labels B and I are then written into the files for later annotaion with inCeption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import shutil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 100 random tsv files and store them\n",
    "def read_random_TSV_files(folder_path, save_folder_path):\n",
    "    \"\"\"\n",
    "    Reads 100 random TSV files from a folder and saves their contents to a new folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the TSV files.\n",
    "        save_folder_path (str): Path to the folder where the new files should be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    files = os.listdir(folder_path)\n",
    "    rs = np.random.RandomState(seed=20)\n",
    "\n",
    "    # randomly select 100 files with the RandomState object\n",
    "    random_files = rs.choice(files, size=50, replace=False)\n",
    "\n",
    "    if not os.path.exists(save_folder_path):\n",
    "        os.makedirs(save_folder_path)\n",
    "\n",
    "    for file_name in random_files:\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_content = file.read()\n",
    "            counter += 1\n",
    "\n",
    "        save_file_path = os.path.join(save_folder_path, file_name)\n",
    "\n",
    "        with open(save_file_path, \"w\") as file:\n",
    "            file.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of folder containing 100 random speeches\n",
    "\n",
    "folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/originalFiles/\"\n",
    "save_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "\n",
    "read_random_TSV_files(folder_path, save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load German spacy model\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "# end = end of a chunk, blank = not at the end of a chunk\n",
    "class LineLabel:\n",
    "    def __init__(self, tag, end, blank):\n",
    "        self.tag = tag\n",
    "        self.end = end\n",
    "        self.blank = blank\n",
    "\n",
    "# return word number (containing a token and its label)\n",
    "def findWordNumber(checkLine):\n",
    "    desiredStart = checkLine.find('-')\n",
    "    desiredEnd = checkLine.find('\\t')\n",
    "\n",
    "    return checkLine[desiredStart + 1:desiredEnd]\n",
    "\n",
    "def determineLabel(line, nounChunkList):\n",
    "    tag = '_'\n",
    "    endOfNounChunk = False\n",
    "\n",
    "    if (line.strip() == ''):\n",
    "        return LineLabel('_', False, True)\n",
    "\n",
    "    # checks if noun chunks are in the list (or not)\n",
    "    if len(nounChunkList) == 0:\n",
    "        return LineLabel('_', False, False)\n",
    "\n",
    "    # identification of the next noun chunk\n",
    "    currentNounChunk = nounChunkList[0]\n",
    "    searchStart = int(currentNounChunk.start) + 1\n",
    "    searchEnd = int(currentNounChunk.end)\n",
    "    wordNumber = int(findWordNumber(line))\n",
    "\n",
    "    if (wordNumber == searchStart):\n",
    "        # Label B (Begin)\n",
    "        tag = 'B'\n",
    "    # checks if current line is in the middle of a noun chunk\n",
    "    if ((searchStart + 1) <= wordNumber <= (searchEnd)):\n",
    "        # Label I (Inside)\n",
    "        tag = 'I'\n",
    "    # checks if current line is the end of a noun chunk\n",
    "    if (wordNumber == searchEnd):\n",
    "        # sets endOfNounChunk to 'True' to hint that the end of a noun chunk is reached\n",
    "        endOfNounChunk = True\n",
    "    return LineLabel(tag, endOfNounChunk, False)\n",
    "\n",
    "# reads all input files\n",
    "def noun_chunk_file(file, output):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        inputLines = f.readlines()\n",
    "\n",
    "    with open(output, 'w', encoding='utf-8') as out:\n",
    "        lineCount = 0\n",
    "        nounChunkList = []\n",
    "\n",
    "        # writes the header of the TSV files in the WebAnno Format\n",
    "        out.writelines(\n",
    "            ['#FORMAT=WebAnno TSV 3.2\\n', '#T_SP=webanno.custom.Animacy|Animated\\n',\n",
    "             '#T_SP=webanno.custom.CustomChunk|Chunk\\n', '\\n'])\n",
    "\n",
    "        for line in inputLines:\n",
    "            lineCount += 1\n",
    "            \n",
    "            # skipps the first lines (header)\n",
    "            if (lineCount < 4):\n",
    "                continue\n",
    "                \n",
    "            # detects lines which contain the text (#Text=...) and updates the noun chunk list(s)\n",
    "            if line[0:6] == '#Text=':\n",
    "                nounChunkList = []\n",
    "                actualText = nlp(line[6:])\n",
    "                nounChunks = actualText.noun_chunks\n",
    "            \n",
    "                for nounChunk in nounChunks:\n",
    "                    nounChunkList.append(nounChunk)\n",
    "                    \n",
    "                out.writelines('\\n' + line)\n",
    "                \n",
    "            else:\n",
    "                # determines the tag and whether a line is the end of a noun chunk or not\n",
    "                label = determineLabel(line, nounChunkList)\n",
    "\n",
    "                if (label.blank):\n",
    "                    out.writelines('')\n",
    "                    continue\n",
    "\n",
    "                # writes the line with the new added tag\n",
    "                lineToWrite = line.rstrip() + '\\t' + label.tag + '\\n'\n",
    "                out.write(lineToWrite)\n",
    "\n",
    "                # removes the noun chunk from the list if line was the last line of a noun chunk\n",
    "                if (label.end):\n",
    "                    nounChunkList.remove(nounChunkList[0])\n",
    "\n",
    "                \n",
    "# folderpaths of the in & output files\n",
    "folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "output_folder_path = \"//Users/lisa/Desktop/Code/Preperation_Of_Classifiers/labeledDataClassifierA/\"\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "for file in files:\n",
    "    noun_chunk_file(folder_path + file, output_folder_path + \"IOB_format_\" + file +\".tsv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f8a85",
   "metadata": {},
   "source": [
    "## Files for Classifier B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing the 100 files\n",
    "folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "test_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "train_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "\n",
    "# percentage of files to use for testing\n",
    "test_percent = 20\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# shuffle the list of files randomly\n",
    "random.shuffle(files)\n",
    "\n",
    "# calculate the number of files to use for testing based on the test_percent\n",
    "num_test_files = int(len(files) * (test_percent / 100))\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "\n",
    "for file in files[:num_test_files]:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    shutil.move(file_path, test_path)\n",
    "\n",
    "for file in files[num_test_files:]:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    shutil.move(file_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_random_TSV_files(folder_path, save_folder_path):\n",
    "    \"\"\"\n",
    "    Reads 80 random TSV files from a folder and saves their contents to a new folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the TSV files.\n",
    "        save_folder_path (str): Path to the folder where the new files should be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    rs = np.random.RandomState(seed=9)\n",
    "\n",
    "    # randomly select 100 files\n",
    "    random_files = rs.choice(files, size=100, replace=False)\n",
    "\n",
    "    if not os.path.exists(save_folder_path):\n",
    "        os.makedirs(save_folder_path)\n",
    "\n",
    "    # loop over the selected files\n",
    "    for file_name in random_files:\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_content = file.read()\n",
    "            counter += 1\n",
    "\n",
    "        save_file_path = os.path.join(save_folder_path, file_name)\n",
    "\n",
    "        with open(save_file_path, \"w\") as file:\n",
    "            file.write(file_content)\n",
    "            \n",
    "            \n",
    "# Creation of folder containing 80 random speeches\n",
    "folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFiles/\"\n",
    "save_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/newRandomTSVFilesForClassifierB\"\n",
    "\n",
    "read_random_TSV_files(folder_path, save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation from TSV WebAnno Files into CoNLL 2000 format for all files for Classifier B\n",
    "\n",
    "\n",
    "# end = end of a chunk, blank = not at the end of a chunk\n",
    "class LineLabel:\n",
    "    def __init__(self, tag, end, blank):\n",
    "        self.tag = tag\n",
    "        self.end = end\n",
    "        self.blank = blank\n",
    "\n",
    "# read all input files\n",
    "def noun_chunk_file(file, output):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        inputLines = f.readlines()\n",
    "\n",
    "    with open(output, 'w', encoding='utf-8') as out:\n",
    "        lineCount = 0\n",
    "        nounChunkList = []\n",
    "\n",
    "        # writes the header of the TSV files in the WebAnno Format\n",
    "        out.writelines(\n",
    "            ['#FORMAT=WebAnno TSV 3.2\\n', '#T_SP=webanno.custom.Animacy|Animated\\n',\n",
    "             '#T_SP=webanno.custom.CustomChunk|Chunk\\n', '\\n'])\n",
    "\n",
    "        for line in inputLines:\n",
    "            lineCount += 1\n",
    "            \n",
    "            # skipps the first lines (header)\n",
    "            if (lineCount < 4):\n",
    "                continue\n",
    "                \n",
    "            # detects lines which contain the text (#Text=...) and updates the noun chunk list(s)\n",
    "            if line[0:6] == '#Text=':\n",
    "                nounChunkList = []\n",
    "                actualText = nlp(line[6:])\n",
    "                nounChunks = actualText.noun_chunks\n",
    "            \n",
    "                for nounChunk in nounChunks:\n",
    "                    nounChunkList.append(nounChunk)\n",
    "                    \n",
    "                # writes the found text (pieces) into the output files\n",
    "                out.writelines('\\n' + line)\n",
    "                \n",
    "            else:\n",
    "                # determines the tag and whether a line is the end of a noun chunk or not\n",
    "                label = determineLabel(line, nounChunkList)\n",
    "\n",
    "                if (label.blank):\n",
    "                    out.writelines('')\n",
    "                    # writes a bank line if the line is empty and continues with the next line in the text\n",
    "                    continue\n",
    "\n",
    "                # writes the line with the new added tag\n",
    "                lineToWrite = line.rstrip() + '\\t' + label.tag + '\\n'\n",
    "                out.write(lineToWrite)\n",
    "\n",
    "                # remove the noun chunk from the list if line was the last line of a noun chunk\n",
    "                if (label.end):\n",
    "                    nounChunkList.remove(nounChunkList[0])        \n",
    "        \n",
    "def findWordNumber(checkLine):\n",
    "    desiredStart = checkLine.find('-')\n",
    "    desiredEnd = checkLine.find('\\t')\n",
    "\n",
    "    return checkLine[desiredStart + 1:desiredEnd]\n",
    "\n",
    "def determineLabel(line, nounChunkList):\n",
    "    tag = '_'\n",
    "    endOfNounChunk = False\n",
    "\n",
    "    # checks if line is blank\n",
    "    if (line.strip() == ''):\n",
    "        return LineLabel('_', False, True)\n",
    "\n",
    "    # checks if noun chunks are in the list (or not)\n",
    "    if len(nounChunkList) == 0:\n",
    "        return LineLabel('_', False, False)\n",
    "\n",
    "    # parameters for the identification of the next noun chunk\n",
    "    currentNounChunk = nounChunkList[0]\n",
    "    searchStart = int(currentNounChunk.start) + 1\n",
    "    searchEnd = int(currentNounChunk.end)\n",
    "    wordNumber = int(findWordNumber(line))\n",
    "\n",
    "    # checks if the current line is the start of a noun chunk\n",
    "    if (wordNumber == searchStart):\n",
    "        # Label B (Begin)\n",
    "        tag = 'B'\n",
    "    # checks if current line is in the middle of a noun chunk\n",
    "    if ((searchStart + 1) <= wordNumber <= (searchEnd)):\n",
    "        # Label I (Inside)\n",
    "        tag = 'I'\n",
    "    # checks if current line is the end of a noun chunk\n",
    "    if (wordNumber == searchEnd):\n",
    "        # sets endOfNounChunk to 'True' to hint that the end of a noun chunk is reached\n",
    "        endOfNounChunk = True\n",
    "    return LineLabel(tag, endOfNounChunk, False)\n",
    "\n",
    "def pos_and_chunk_tags(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    chunk_tags = ['O'] * len(doc)\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        for i in range(chunk.start, chunk.end):\n",
    "            if i == chunk.start:\n",
    "                chunk_tags[i] = 'B-NP'\n",
    "            else:\n",
    "                chunk_tags[i] = 'I-NP'\n",
    "\n",
    "    return pos_tags, chunk_tags\n",
    "    \n",
    "\n",
    "# transform webanno to conll2000\n",
    "def webanno_tsv_to_conll2000_with_spacy(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as tsvfile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        sentence_started = False\n",
    "        sentence = []\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) == 1 and row[0].startswith('#Text='):\n",
    "                if sentence_started:\n",
    "                    # process the sentence\n",
    "                    pos_tags, chunk_tags = pos_and_chunk_tags(\" \".join(sentence))\n",
    "\n",
    "                    # write the sentence in CoNLL 2000 format\n",
    "                    for (word, pos_tag), chunk_tag in zip(pos_tags, chunk_tags):\n",
    "                        outfile.write(f\"{word} {pos_tag} {chunk_tag}\\n\")\n",
    "\n",
    "                    # add an empty line to separate sentences\n",
    "                    outfile.write(\"\\n\")\n",
    "\n",
    "                    # reset the sentence\n",
    "                    sentence = []\n",
    "\n",
    "                sentence_started = True\n",
    "\n",
    "            elif len(row) >= 3 and sentence_started:\n",
    "                word = row[2]\n",
    "                sentence.append(word)\n",
    "\n",
    "        # process and write the last sentence\n",
    "        if sentence:\n",
    "            pos_tags, chunk_tags = pos_and_chunk_tags(\" \".join(sentence))\n",
    "\n",
    "            for (word, pos_tag), chunk_tag in zip(pos_tags, chunk_tags):\n",
    "                outfile.write(f\"{word} {pos_tag} {chunk_tag}\\n\")\n",
    "\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "# folder paths of the input and output files\n",
    "input_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/originalFiles/\n",
    "output_conll2000_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/All Files/\"\n",
    "output_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/IOB Format/\"\n",
    "\n",
    "input_files = os.listdir(input_folder_path)\n",
    "\n",
    "for input_file in input_files:\n",
    "    # only process .tsv files\n",
    "    if input_file.endswith(\".csv\"):\n",
    "        # output file paths\n",
    "        output_conll2000_file = output_conll2000_folder_path + \"CoNNL2000_\" + input_file[:-4] + \".txt\"\n",
    "        output_tsv_file = output_folder_path + \"IOB_format_\" + input_file\n",
    "        print(output_tsv_file)\n",
    "        # apply the noun_chunk_file function to the input TSV file\n",
    "        noun_chunk_file(input_folder_path + input_file, output_tsv_file)\n",
    "        print(noun_chunk_file)\n",
    "        # convert the updated TSV file to the CoNLL 2000 format\n",
    "        webanno_tsv_to_conll2000_with_spacy(output_tsv_file, output_conll2000_file)\n",
    "        print(webanno_tsv_to_conll2000_with_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of 20:80 validation and training split and store in folders\n",
    "\n",
    "def split_data_into_train_and_validation(input_folder, train_folder, validation_folder, validation_size=20):\n",
    "    all_files = os.listdir(input_folder)\n",
    "    validation_files = random.sample(all_files, validation_size)\n",
    "    train_files = [file for file in all_files if file not in validation_files]\n",
    "\n",
    "    for file in validation_files:\n",
    "        shutil.copy(os.path.join(input_folder, file), os.path.join(validation_folder, file))\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(input_folder, file), os.path.join(train_folder, file))\n",
    "\n",
    "input_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/All Files/\"\n",
    "train_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/Training/\"\n",
    "validation_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/Validation/\"\n",
    "\n",
    "os.makedirs(train_folder_path, exist_ok=True)\n",
    "os.makedirs(validation_folder_path, exist_ok=True)\n",
    "\n",
    "split_data_into_train_and_validation(input_folder_path, train_folder_path, validation_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform conll2000 files to dataframe\n",
    "\n",
    "def conll2000_to_dataframe(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = content.split('\\n\\n')\n",
    "\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            lines = sentence.split('\\n')\n",
    "            for line in lines:\n",
    "                tokens = line.split()\n",
    "                if len(tokens) == 3:\n",
    "                    data.append(tokens)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Word', 'POS', 'Chunk'])\n",
    "    return df\n",
    "\n",
    "def read_files_to_dataframe(folder_path, file_names):\n",
    "    dfs = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = conll2000_to_dataframe(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# folder paths of the input files\n",
    "train_input_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/Training/\"\n",
    "test_input_folder_path = \"/Users/lisa/Desktop/Code/Preperation_Of_Classifiers/CoNLL Format/Validation/\"\n",
    "\n",
    "# get all CoNLL 2000 files in the input folders\n",
    "train_files = [f for f in os.listdir(train_input_folder_path) if f.endswith('.txt')]\n",
    "test_files = [f for f in os.listdir(test_input_folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# read the training and testing data into DataFrames\n",
    "classifier_B_train_df = read_files_to_dataframe(train_input_folder_path, train_files)\n",
    "classifier_B_validation_df = read_files_to_dataframe(test_input_folder_path, test_files)\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(classifier_B_train_df.head())\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(classifier_B_validation_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
